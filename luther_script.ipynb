{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Luther: Predicting the Market Value of NBA Players\n",
    "\n",
    "Name: Paul Lim\n",
    "\n",
    "Date: 04/28/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import pipeline, feature_selection, decomposition\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import RFECV\n",
    "import logging\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "sns.set_style({'legend.frameon': True})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Classes/Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bball_get_col(table, col_tag='th'):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Generate the list of column names from the HTML table.\n",
    "    INPUT:\n",
    "        - table is an HTML table found on basketball-reference.com\n",
    "        - col_tag is set to th as default. \n",
    "    OUTPUT:\n",
    "        - list of column names is outputted\n",
    "    '''\n",
    "    col_loc = table.find('tr')\n",
    "    cols = col_loc.find_all(col_tag)\n",
    "\n",
    "    cols_list = []\n",
    "    for i in range(len(cols)):\n",
    "        temp_col = cols[i].get_text()\n",
    "        cols_list.append(temp_col)\n",
    "        \n",
    "    return cols_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bball_get_data(table):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Aggregate the data from the HTML table.\n",
    "    INPUT:\n",
    "        - table is an HTML table found on basketball-reference.com\n",
    "    OUTPUT:\n",
    "        - data from the HTML table is outputted as a list of lists.\n",
    "    '''\n",
    "    all_rows = table.find_all('tr')[1:]\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(len(all_rows)):\n",
    "        row = all_rows[i].find_all('th')\n",
    "        add_row = all_rows[i].find_all('td')\n",
    "        row.extend(add_row)\n",
    "        data = []\n",
    "        for j in range(len(row)):\n",
    "            datapoint = row[j].get_text()\n",
    "            data.append(datapoint)\n",
    "        all_data.append(data)\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bball_scrape_data(url_template, start_year, end_year, delay=5):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Creates a pandas dataframe object from the data scraped from basketball-reference.com.\n",
    "    INPUT:\n",
    "        - url_template is the url that contains the data table that will be scraped.\n",
    "        - start_year and end_year give the range of years to scrape.\n",
    "        - delay is the number of seconds in-between moving on to the next url.\n",
    "    OUTPUT:\n",
    "        - data from the HTML table is outputted as a dataframe.\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_year > end_year:\n",
    "        return print('Enter in valid end year.')\n",
    "    else: \n",
    "        for year in range(start_year, end_year+1):\n",
    "            try:\n",
    "                url = url_template.format(year=year)\n",
    "                link = requests.get(url)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('Check to make sure the URL is correct!')\n",
    "            \n",
    "            page = link.text\n",
    "\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            col_list = bball_get_col(table)\n",
    "            all_data = bball_get_data(table)\n",
    "            \n",
    "            if not col_list or not all_data:\n",
    "                print(url)\n",
    "                print('Webpage may be empty.')\n",
    "                pass\n",
    "            elif len(col_list) != len(all_data[0]):\n",
    "                print('Column List: \\n', col_list)\n",
    "                print('Data Row: \\n', all_data)\n",
    "                return print('Make sure the length of columns and data are consistent!')\n",
    "            else:\n",
    "                temp_df = pd.DataFrame(all_data, columns=col_list)\n",
    "                temp_df = temp_df.assign(Yr = year)\n",
    "                df = df.append(temp_df)\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sal_get_col(table_sal):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Generate the column names for the salary table.\n",
    "    INPUT:\n",
    "        - table_sal is the HTML table from ESPN.com.\n",
    "    OUTPUT:\n",
    "        - list of column names is outputted.\n",
    "    '''\n",
    "    col_loc_sal = table_sal.find('tr')\n",
    "    cols_sal = col_loc_sal.find_all('td')\n",
    "\n",
    "    cols_list_sal = []\n",
    "    for i in range(len(cols_sal)):\n",
    "        temp_col_sal = cols_sal[i].get_text()\n",
    "        cols_list_sal.append(temp_col_sal)\n",
    "    return cols_list_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sal_get_data(table_sal):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Aggregate the data from the salary table.\n",
    "    INPUT:\n",
    "        - table_sal is the HTML table from ESPN.com.\n",
    "    OUTPUT:\n",
    "        - data from the salary table is outputted as a list of lists.\n",
    "    '''\n",
    "    all_rows_sal = table_sal.find_all('tr', class_ = ['evenrow', 'oddrow'])\n",
    "\n",
    "    all_data_sal = []\n",
    "\n",
    "    for i in range(len(all_rows_sal)):\n",
    "        row_sal = all_rows_sal[i].find_all('td')\n",
    "        data_sal = []\n",
    "        for j in range(len(row_sal)):\n",
    "            datapoint_sal = row_sal[j].get_text()\n",
    "            data_sal.append(datapoint_sal)\n",
    "        all_data_sal.append(data_sal)\n",
    "        \n",
    "    return all_data_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sal_scrape_data(url_template, start_year, end_year, start_page, end_page, delay=5):\n",
    "     '''\n",
    "    DESCRIPTION:\n",
    "        - Creates a pandas dataframe object from the salary table on ESPN.com.\n",
    "    INPUT:\n",
    "        - url_template is the url that contains the data table that will be scraped.\n",
    "        - start_year and end_year give the range of years to scrape.\n",
    "        - start_page and end_page give the number of pages available for each year.\n",
    "        - delay is the number of seconds in-between moving on to the next url.\n",
    "    OUTPUT:\n",
    "        - data from the HTML table is outputted as a dataframe.\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_year > end_year:\n",
    "        return print('Enter in valid end year.')\n",
    "    elif start_page > end_page:\n",
    "        return pring('Enter in a valid end page.')\n",
    "    else: \n",
    "        for year in range(start_year, end_year+1):\n",
    "            for page in range(start_page, end_page+1):\n",
    "                try:\n",
    "                    url = url_template.format(year=year, page=page)\n",
    "                    link = requests.get(url)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('Check to make sure the URL is correct!')\n",
    "                \n",
    "                page = link.text\n",
    "\n",
    "                soup = BeautifulSoup(page, 'lxml')\n",
    "                table = soup.find('table')\n",
    "\n",
    "                col_list = sal_get_col(table)\n",
    "                all_data = sal_get_data(table)\n",
    "                \n",
    "                if not col_list or not all_data:\n",
    "                    print(url)\n",
    "                    print('Webpage may be empty.')\n",
    "                    pass\n",
    "                elif len(col_list) != len(all_data[0]):\n",
    "                    print('Column List: \\n', col_list)\n",
    "                    print('Data Row: \\n', all_data)\n",
    "                    return print('Make sure the length of columns and data are consistent!')\n",
    "                else:\n",
    "                    temp_df = pd.DataFrame(all_data, columns=col_list)\n",
    "                    temp_df = temp_df.assign(Yr = year)\n",
    "                    df = df.append(temp_df)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_player_data(df_list):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Clean the messy data that was scraped from basketball-reference.com.\n",
    "    INPUT:\n",
    "        - df_list is a list of dataframes containing scraped data.\n",
    "    OUTPUT:\n",
    "        - clean_df_list is a list of clean dataframes.\n",
    "    '''\n",
    "    clean_df_list = []\n",
    "    for df in df_list:\n",
    "        # Remove the Unnamed and Rk columns\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop('Unnamed', axis=1)\n",
    "        if 'Rk' in df.columns:\n",
    "            df = df.drop('Rk', axis=1)\n",
    "        \n",
    "        # Remove the rows there the Tm column value is 'Tm'.\n",
    "        df = df[df['Tm'] != 'Tm']\n",
    "        \n",
    "        # Remove characters such as asteriks from player names.\n",
    "        df['Player'] = df['Player'].map(lambda x: re.sub(r\"\\*\", '', x))\n",
    "        \n",
    "        # Some players played on multiple teams in the same year due to trades.\n",
    "        # These players will have their stats averaged to one value.\n",
    "        df = df.sort(columns=['Player','Yr','Pos','Tm'])\n",
    "        df = df.apply(pd.to_numeric, errors='ignore')\n",
    "        df = df.groupby(['Player','Yr','Pos','Tm']).mean().reset_index()\n",
    "        \n",
    "        clean_df_list.append(df)\n",
    "    return clean_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_salary_data(df):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Clean the messy data that was scraped from ESPN.com.\n",
    "    INPUT:\n",
    "        - df is a dataframe containing scraped data.\n",
    "    OUTPUT:\n",
    "        - clean_df is a clean dataframe.\n",
    "    '''\n",
    "    # Remove the Unnamed and Rk columns\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop('Unnamed', axis=1)\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df.drop('Rk', axis=1)\n",
    "        \n",
    "    # Edit the Name column so that only the names are left.\n",
    "    df['NAME'] = df['NAME'].map(lambda x: re.sub(r\",\\s.+\", '', x))\n",
    "    \n",
    "    # Edit the Salary column to an integer value.\n",
    "    df['SALARY'] = df['SALARY'].map(lambda x: re.sub(r\"\\$\", '', x))\n",
    "    df['SALARY'] = df['SALARY'].map(lambda x: re.sub(r\",\", '', x))\n",
    "    df['SALARY'] = df['SALARY'].map(lambda x: int(x))\n",
    "    \n",
    "    # Adjust the Salary column to account for inflation.\n",
    "    # Average rate of inflation between 2000 and 2016 was 2.15%\n",
    "    df['yr_diff'] = df['Yr'].map(lambda x: 2017 - x)\n",
    "    df['salary_adj'] = df.apply(lambda x: x['SALARY']*(1 + 0.0215)**x['yr_diff'], axis=1).map(int)\n",
    "    \n",
    "    # Ultimately the salary dataframe will only need the Name, Year, Salary, and Adjusted Salary columns. \n",
    "    # The Name and Yr columns will be used as the key when joining the two databases.\n",
    "    df = df.loc[:,['NAME','Yr', 'SALARY', 'salary_adj']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_df(df_tot, df_adv, df_sal):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Merge the total stat, advanced stat, and salary dataframes.\n",
    "    INPUT:\n",
    "        - df_tot is the total stat dataframe.\n",
    "        - df_adv is the advanced stat dataframe.\n",
    "        - df_sal is the salary dataframe.\n",
    "    OUTPUT:\n",
    "        - df_merge is the merged dataframe.\n",
    "    '''\n",
    "    # Since df_tot and df_adv have duplicate columns these columns will be excluded when merged.\n",
    "    col_to_use = df_tot.columns - df_adv.columns\n",
    "    df_merge = df_adv.merge(df_tot[col_to_use], how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # The salary dataframe and the combined dataframe from above should be joined on the player name and year.\n",
    "    df_merge = df_sal.merge(df_merge, how='inner', left_on=['NAME', 'Yr'], right_on=['Player', 'Yr'])\n",
    "    df_merge = df_merge.drop('Player', axis=1)\n",
    "    \n",
    "    # Replace null values with 0\n",
    "    df_merge['3P%'] = df_merge.loc[:,'3P%'].fillna(0)\n",
    "    df_merge['2P%'] = df_merge.loc[:, '2P%'].fillna(0)\n",
    "    df_merge['FT%'] = df_merge.loc[:, 'FT%'].fillna(0)\n",
    "    df_merge['3PAr'] = df_merge.loc[:, '3PAr'].fillna(0)\n",
    "    df_merge['FTr'] = df_merge.loc[:, 'FTr'].fillna(0)\n",
    "    df_merge['FG%'] = df_merge.loc[:, 'FG%'].fillna(0)\n",
    "    df_merge['eFG%'] = df_merge.loc[:, 'eFG%'].fillna(0)\n",
    "    \n",
    "    # Create dummy variables for position.\n",
    "    pos_dict = {'C': 'C', 'PF': 'PF','SF': 'SF','PG':'PG','SG':'SG','C-PF': 'C','SF-SG':'SF','PG-SG':'PG',\n",
    "               'PG-SF':'PG','SG-SF':'SG','PF-C':'PF','PF-SF':'PF','SG-PG':'SG','SG-PF':'SG','SF-PF':'SF'}\n",
    "    df_merge['Pos'] = df_merge['Pos'].map(pos_dict)\n",
    "    df_merge = pd.get_dummies(df_merge, columns=['Pos'])\n",
    "    \n",
    "    # There are columns with all null values. These columns will be dropped.\n",
    "    df_merge = df_merge.dropna(axis=1, how='all')\n",
    "    df_merge = df_merge.sample(frac=1)\n",
    "    \n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_features(df, col_list):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Square certain features to create new features.\n",
    "    INPUT:\n",
    "        - df is the input dataframe.\n",
    "        - col_list is the list of columns that need to be squared.\n",
    "    OUTPUT:\n",
    "        - df is the output dataframe with new squared features.\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        new_col_name = col + '_sq'\n",
    "        df[new_col_name] = df[col].apply(lambda x: x**2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cube_features(df, col_list):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Cube certain features to create new features.\n",
    "    INPUT:\n",
    "        - df is the input dataframe.\n",
    "        - col_list is the list of columns that need to be cubed.\n",
    "    OUTPUT:\n",
    "        - df is the output dataframe with new cubed features.\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        new_col_name = col + '_cube'\n",
    "        df[new_col_name] = df[col].apply(lambda x: x**3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_features(df, groupby_list, rank_list):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - Rank players on each team for certain stats to create new features.\n",
    "    INPUT:\n",
    "        - df is the input dataframe.\n",
    "        - groupby_list is the list of features to group the input dataframe by.\n",
    "        - rank_list is the list of columns that will be used to rank players.\n",
    "    OUTPUT:\n",
    "        - df is the output dataframe with new cubed features.\n",
    "    '''\n",
    "    df_rank = df.groupby(groupby_list).rank(ascending=True)\n",
    "    new_col_list = []\n",
    "    for col in rank_list:\n",
    "        new_col_name = col + '_rk'\n",
    "        df_rank[new_col_name] = df_rank[col]\n",
    "        new_col_list.append(new_col_name)\n",
    "    return df_rank[new_col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, ratios=True, rank=True):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "        - New features were created after preliminary data analysis. These new features may or may not be used.\n",
    "    INPUT:\n",
    "        - df is the input dataframe.\n",
    "        - ratios denotes if ratio of stats should be created.\n",
    "        - rank denotes if players on teams should be ranked by certain stats.\n",
    "    OUTPUT:\n",
    "        - df is the output dataframe with new squared features.\n",
    "    '''\n",
    "    # Square PER, AGE, GS, G, USG%\n",
    "    df = square_features(df, ['PER','Age','GS','G','USG%'])\n",
    "\n",
    "    # Cube Age, G, USG%\n",
    "    df = cube_features(df, ['Age','G','USG%'])\n",
    "    \n",
    "    if ratios:\n",
    "        # Create ratios: PTS/TOV, AST/TOV, STL/TOV, BLK/TRB, STL/AST\n",
    "        df['PTS/TOV'] = (df['PTS']/df['TOV']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['AST/TOV'] = (df['AST']/df['TOV']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['STL/TOV'] = (df['STL']/df['TOV']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['BLK/TRB'] = (df['BLK']/df['TRB']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['STL/AST'] = (df['STL']/df['AST']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "\n",
    "        # Create ratios: WS/PER, OWS/PER, WS/OWS, BPM/OWS, BPM/WSper48\n",
    "        df['WS/PER'] = (df['WS']/df['PER']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['OWS/PER'] = (df['OWS']/df['PER']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['WS/OWS'] = (df['WS']/df['OWS']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['BPM/OWS'] = (df['BPM']/df['OWS']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        df['BPM/WSper48'] = (df['BPM']/df['WS/48']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "        \n",
    "    if rank:\n",
    "        # Create features that will rank players in each team and each year.\n",
    "        rank_col_list = ['PTS','AST','TRB','BLK','STL','TOV','MP','WS','PER']\n",
    "        group_col_list = ['Yr','Tm']\n",
    "    \n",
    "        df_rank = rank_features(df, group_col_list, rank_col_list)\n",
    "        df = df.merge(df_rank, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Change target feature to log(salary_adj)\n",
    "    df['salary_adj_ln'] = df['salary_adj'].apply(np.log)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_pipelines(model_dict, X, y, feature_list, param_dict):\n",
    "    '''\n",
    "    Runs through a pipeline for each type of model.\n",
    "    feature_list = list of tuples.\n",
    "    param_dict = a nested dictionary that contains the hyper parameters that need to be tuned.\n",
    "    '''\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "    grid_dict = {}\n",
    "    \n",
    "    for name, model in sorted(model_dict.items()):\n",
    "        if feature_list:\n",
    "            combined_features = pipeline.FeatureUnion(feature_list)\n",
    "\n",
    "            steps = [\n",
    "                ('features', combined_features),\n",
    "                ('feature_selection', feature_selection.SelectFromModel(linear_model.Lasso(alpha=2))),\n",
    "                ('model', model)\n",
    "                ]\n",
    "        else:\n",
    "            steps = [\n",
    "                ('feature_selection', feature_selection.SelectFromModel(linear_model.Lasso(alpha=2))),\n",
    "                ('model', model)\n",
    "                ]\n",
    "        \n",
    "        regression_pipeline = pipeline.Pipeline(steps)\n",
    "        if name in param_dict:\n",
    "            parameters = param_dict[name]\n",
    "        else:\n",
    "            return print('Incorrect parameters in the parameter dictionary.')\n",
    "            \n",
    "        grid_dict[name] = GridSearchCV(regression_pipeline, parameters, n_jobs=3, verbose=1)\n",
    "        train_fit = grid_dict[name].fit(X_train, y_train)\n",
    "        print('Model: ', name)\n",
    "        print('Best Score: %0.3f' % train_fit.best_score_)\n",
    "        print('Optimal Parameters: ', train_fit.best_params_)\n",
    "         \n",
    "    return grid_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_ref_url = 'http://www.basketball-reference.com/leagues/NBA_{year}_advanced.html'\n",
    "bball_tot_url = 'http://www.basketball-reference.com/leagues/NBA_{year}_totals.html'\n",
    "bball_per_url = 'http://www.basketball-reference.com/leagues/NBA_{year}_per_game.html'\n",
    "salary_url = 'http://www.espn.com/nba/salaries/_/year/{year}/page/{page}/seasontype/3'\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2017\n",
    "start_page = 1\n",
    "end_page = 4\n",
    "\n",
    "df_bball = bball_scrape_data(bball_ref_url, start_year, end_year)\n",
    "df_bball_tot = bball_scrape_data(bball_tot_url, start_year, end_year)\n",
    "df_bball_per = bball_scrape_data(bball_per_url, start_year, end_year)\n",
    "df_sal = sal_scrape_data(salary_url, start_year, end_year, start_page, end_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df_list = clean_player_data([df_bball, df_bball_tot])\n",
    "df_salary = clean_salary_data(df_sal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join dataframes on a key formed by the year and name of the player.\n",
    "Inner join will be used since the players that don't have salary information will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merge = merge_df(clean_df_list[1], clean_df_list[0], df_sal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merge = feature_engineering(df_merge)\n",
    "\n",
    "X_cols = ['Age', 'G',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'BLK', 'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'PTS', 'STL', 'TOV', 'TRB', 'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG',\n",
    "       'Pos_SF', 'Pos_SG', 'PER_sq', 'Age_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk','WS_rk','PER_rk']\n",
    "\n",
    "# Create the X and y matrices. \n",
    "# Also, set apart 15% of the data for a holdout set.\n",
    "holdout = df_merge.iloc[0:403, :]\n",
    "\n",
    "df_test = df_merge.iloc[403:,:]\n",
    "\n",
    "X = df_test.iloc[403:,:]\n",
    "X = X.loc[:, X_cols]\n",
    "y = df_test.iloc[403:,:]\n",
    "y = y.loc[:, 'salary_adj']\n",
    "\n",
    "X_arr = X.as_matrix()\n",
    "y_arr = y.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pipelines to choose the best model with tuned hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_dict = {}\n",
    "pipe_dict['gradient_boost'] = GradientBoostingRegressor()\n",
    "pipe_dict['random_forest'] = RandomForestRegressor()\n",
    "pipe_dict['lin_reg'] = linear_model.LinearRegression()\n",
    "pipe_dict['lasso'] = linear_model.Lasso()\n",
    "pipe_dict['ridge'] = linear_model.Ridge()\n",
    "\n",
    "\n",
    "param_dict = {}\n",
    "param_dict['gradient_boost'] = {'model__n_estimators': [10,100,300], \n",
    "                              'model__max_depth': [2,3,4,5], \n",
    "                              'model__min_samples_split': [5,10,20,40],\n",
    "                              'model__max_features': ['sqrt', 'auto']}\n",
    "param_dict['random_forest'] = {'model__n_estimators': [10,100,300],\n",
    "                             'model__max_depth': [2,3,4,5], \n",
    "                             'model__min_samples_split': [5,10,20,40],\n",
    "                             'model__max_features': ['sqrt', 'auto']}\n",
    "param_dict['lin_reg'] = {'model__fit_intercept': [True]}\n",
    "param_dict['lasso'] = {'model__alpha': [.01,.1,1,10,100]}\n",
    "param_dict['ridge'] = {'model__alpha' : [.01,.1,1,10,100]}\n",
    "param_dict['elastic'] = {'model__alpha': [.01,.1,1,10,100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = run_pipelines(pipe_dict, X, y, feature_list=None, param_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
