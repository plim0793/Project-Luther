{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Luther: Predicting the Market Value of NBA Players\n",
    "\n",
    "Name: Paul Lim\n",
    "\n",
    "Date: 04/28/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import pipeline, feature_selection, decomposition\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import RFECV\n",
    "import logging\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Classes/Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running list of classes/functions\n",
    "\n",
    "class IdentityTransform(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Maintains the original matrix of features.\n",
    "    '''\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "def bball_get_col(table, col_tag='th'):\n",
    "    '''\n",
    "    The columns of the table should be in the first 'tr' tag within the table tag.  \n",
    "    The columns will be found as either 'th' or 'td' tags within the 'tr' tag.  \n",
    "    The 'th' tag is set as default.\n",
    "    '''\n",
    "    col_loc = table.find('tr')\n",
    "    cols = col_loc.find_all(col_tag)\n",
    "\n",
    "    cols_list = []\n",
    "    for i in range(len(cols)):\n",
    "        temp_col = cols[i].get_text()\n",
    "        cols_list.append(temp_col)\n",
    "        \n",
    "    return cols_list\n",
    "\n",
    "def bball_get_data(table, data_tag='td'):\n",
    "    '''\n",
    "    The data within a table should be the second through last 'tr' tag within the table tag.\n",
    "    The data should be found as 'td' tags, so the default data tag is set to 'td'.\n",
    "    '''\n",
    "    all_rows = table.find_all('tr')[1:]\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(len(all_rows)):\n",
    "        row = all_rows[i].find_all('th')\n",
    "        add_row = all_rows[i].find_all('td')\n",
    "        row.extend(add_row)\n",
    "        data = []\n",
    "        for j in range(len(row)):\n",
    "            datapoint = row[j].get_text()\n",
    "            data.append(datapoint)\n",
    "        all_data.append(data)\n",
    "        \n",
    "    return all_data\n",
    "\n",
    "def bball_scrape_data(url_template, start_year, end_year, delay=5):\n",
    "    '''\n",
    "    The url_template should be a formatted string where the start_year and end_year can be cycled through.\n",
    "    The start_year and end_year should be integers.\n",
    "    The col_list should be a list.\n",
    "    The default data_tag is 'td'.\n",
    "    The delay, which delays each run through of the for loop, is set to 5 seconds as default.  \n",
    "    The delay prevents the scraper from being blocked due to frequent scraping requests.\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_year > end_year:\n",
    "        return print('Enter in valid end year.')\n",
    "    else: \n",
    "        for year in range(start_year, end_year+1):\n",
    "            try:\n",
    "                url = url_template.format(year=year)\n",
    "                link = requests.get(url)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('Check to make sure the URL is correct!')\n",
    "            \n",
    "            print(url)\n",
    "            page = link.text\n",
    "\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            col_list = bball_get_col(table)\n",
    "            all_data = bball_get_data(table)\n",
    "            \n",
    "            if not col_list or not all_data:\n",
    "                print(url)\n",
    "                print('Webpage may be empty.')\n",
    "                pass\n",
    "            elif len(col_list) != len(all_data[0]):\n",
    "                print('Column List: \\n', col_list)\n",
    "                print('Data Row: \\n', all_data)\n",
    "                return print('Make sure the length of columns and data are consistent!')\n",
    "            else:\n",
    "                temp_df = pd.DataFrame(all_data, columns=col_list)\n",
    "                temp_df = temp_df.assign(Yr = year)\n",
    "                df = df.append(temp_df)\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sal_get_col(table_sal):\n",
    "    '''\n",
    "    Column headers were found in the 'td' tags within the first 'tr' tag.\n",
    "    '''\n",
    "    col_loc_sal = table_sal.find('tr')\n",
    "    cols_sal = col_loc_sal.find_all('td')\n",
    "\n",
    "    cols_list_sal = []\n",
    "    for i in range(len(cols_sal)):\n",
    "        temp_col_sal = cols_sal[i].get_text()\n",
    "        cols_list_sal.append(temp_col_sal)\n",
    "    return cols_list_sal\n",
    "\n",
    "def sal_get_data(table_sal):\n",
    "    '''\n",
    "    All of the data were found in the 'td' tags within the 'tr' tags that had the class 'evenrow' or 'oddrow'.\n",
    "    '''\n",
    "    all_rows_sal = table_sal.find_all('tr', class_ = ['evenrow', 'oddrow'])\n",
    "\n",
    "    all_data_sal = []\n",
    "\n",
    "    for i in range(len(all_rows_sal)):\n",
    "        row_sal = all_rows_sal[i].find_all('td')\n",
    "        data_sal = []\n",
    "        for j in range(len(row_sal)):\n",
    "            datapoint_sal = row_sal[j].get_text()\n",
    "            data_sal.append(datapoint_sal)\n",
    "        all_data_sal.append(data_sal)\n",
    "        \n",
    "    return all_data_sal\n",
    "\n",
    "def sal_scrape_data(url_template, start_year, end_year, start_page, end_page, delay=5):\n",
    "    '''\n",
    "    Function to scrape the salary data from ESPN.com. The url_template should be a formattable\n",
    "    string that takes in the start_year through the end_year. Since each year had multiple pages\n",
    "    the url_template should also be able to take in the start_page through the end_page.\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_year > end_year:\n",
    "        return print('Enter in valid end year.')\n",
    "    elif start_page > end_page:\n",
    "        return print('Enter in a valid end page.')\n",
    "    else: \n",
    "        for year in range(start_year, end_year+1):\n",
    "            for page in range(start_page, end_page+1):\n",
    "                try:\n",
    "                    url = url_template.format(year=year, page=page)\n",
    "                    link = requests.get(url)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('Check to make sure the URL is correct!')\n",
    "                \n",
    "                page = link.text\n",
    "\n",
    "                soup = BeautifulSoup(page, 'lxml')\n",
    "                table = soup.find('table')\n",
    "\n",
    "                col_list = sal_get_col(table)\n",
    "                all_data = sal_get_data(table)\n",
    "                \n",
    "                if not col_list or not all_data:\n",
    "                    print(url)\n",
    "                    print('Webpage may be empty.')\n",
    "                    pass\n",
    "                elif len(col_list) != len(all_data[0]):\n",
    "                    print('Column List: \\n', col_list)\n",
    "                    print('Data Row: \\n', all_data)\n",
    "                    return print('Make sure the length of columns and data are consistent!')\n",
    "                else:\n",
    "                    temp_df = pd.DataFrame(all_data, columns=col_list)\n",
    "                    temp_df = temp_df.assign(Yr = year)\n",
    "                    df = df.append(temp_df)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "    return df\n",
    "\n",
    "\n",
    "def square_features(df, col_list):\n",
    "    '''\n",
    "    Function that creates the squared term of the features in the col_list.\n",
    "    The dataframe that is returned contains the original features with the additional squared features.\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        new_col_name = col + '_sq'\n",
    "        df[new_col_name] = df[col].apply(lambda x: x**2)\n",
    "    return df\n",
    "\n",
    "def cube_features(df, col_list):\n",
    "    '''\n",
    "    Function that creates the cubed term of the features in the col_list.\n",
    "    The dataframe that is returned contains the original features with the additional cubed features.\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        new_col_name = col + '_cube'\n",
    "        df[new_col_name] = df[col].apply(lambda x: x**3)\n",
    "    return df\n",
    "\n",
    "def rank_features(df, groupby_list, rank_list):\n",
    "    '''\n",
    "    Function that ranks the players in the particular groupby object specified by the groupby_list.\n",
    "    The dataframe that is returned contains the original features with the additional ranked features.\n",
    "    '''\n",
    "    df_rank = df.groupby(groupby_list).rank(ascending=True)\n",
    "    new_col_list = []\n",
    "    for col in rank_list:\n",
    "        new_col_name = col + '_rk'\n",
    "        df_rank[new_col_name] = df_rank[col]\n",
    "        new_col_list.append(new_col_name)\n",
    "    return df_rank[new_col_list]\n",
    "\n",
    "\n",
    "def rand_features(df, col_names):\n",
    "    '''\n",
    "    Function that creates noisy features that contain integers between 0 and 10000.\n",
    "    The dataframe that is returned contains the original dataframe with the additional noisy features.\n",
    "    '''\n",
    "    for col in col_names:\n",
    "        df[col] = np.random.randint(0,10000,len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_pipelines(model_dict, X, y, feature_list, param_dict):\n",
    "    '''\n",
    "    Runs through a pipeline for each type of model.\n",
    "    feature_list = list of tuples.\n",
    "    param_dict = a nested dictionary that contains the hyper parameters that need to be tuned.\n",
    "    '''\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "    grid_dict = {}\n",
    "    \n",
    "    for name, model in sorted(model_dict.items()):\n",
    "        if feature_list:\n",
    "            combined_features = pipeline.FeatureUnion(feature_list)\n",
    "\n",
    "            steps = [\n",
    "                ('features', combined_features),\n",
    "                ('feature_selection', feature_selection.SelectFromModel(linear_model.Lasso(alpha=2))),\n",
    "                ('model', model)\n",
    "                ]\n",
    "        else:\n",
    "            steps = [\n",
    "                ('feature_selection', feature_selection.SelectFromModel(linear_model.Lasso(alpha=2))),\n",
    "                ('model', model)\n",
    "                ]\n",
    "        \n",
    "        regression_pipeline = pipeline.Pipeline(steps)\n",
    "        if name in param_dict:\n",
    "            parameters = param_dict[name]\n",
    "        else:\n",
    "            return print('Incorrect parameters in the parameter dictionary.')\n",
    "            \n",
    "        grid_dict[name] = GridSearchCV(regression_pipeline, parameters, n_jobs=3, verbose=1)\n",
    "        train_fit = grid_dict[name].fit(X_train, y_train)\n",
    "        print('Model: ', name)\n",
    "        print('Best Score: %0.3f' % train_fit.best_score_)\n",
    "        print('Optimal Parameters: ', train_fit.best_params_)\n",
    "         \n",
    "    return grid_dict\n",
    "\n",
    "\n",
    "def fit_models(model_dict, df, col_list, target):\n",
    "    '''\n",
    "    Similar to the run_pipelines function. This function runs a 5-fold cross-validation on the models\n",
    "    defined in the model_dict. The models are fit to the training and test sets from the df and the target\n",
    "    that is specified in the inputs.\n",
    "    '''\n",
    "    X = df.loc[:,col_list]\n",
    "    y = df.loc[:,target]\n",
    "    \n",
    "    X_arr = X.as_matrix()\n",
    "    y_arr = y.as_matrix()\n",
    "    \n",
    "    list_scores = []\n",
    "    list_dict = []\n",
    "    total_model_dict = {}\n",
    "    kf = KFold(n=len(X_arr), n_folds=5)\n",
    "    for name, model in sorted(model_dict.items()):\n",
    "\n",
    "        temp_score_list = []\n",
    "        temp_train_score_list = []\n",
    "        for split_indices in kf:\n",
    "            train_ind = list(split_indices[0])\n",
    "            test_ind = list(split_indices[1])\n",
    "            \n",
    "            X_train = X_arr[train_ind]\n",
    "            X_test = X_arr[test_ind]\n",
    "            y_train = y_arr[train_ind]\n",
    "            y_test = y_arr[test_ind]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            if hasattr(model, 'score'):\n",
    "                temp_score = model.score(X_test, y_test)\n",
    "                temp_score_list.append(temp_score)\n",
    "            \n",
    "                temp_train_score = model.score(X_train, y_train)\n",
    "                temp_train_score_list.append(temp_train_score)\n",
    "            elif hasattr(model, 'score_'):\n",
    "                temp_score = model.score_(X_test, y_test)\n",
    "                temp_score_list.append(temp_score)\n",
    "            \n",
    "                temp_train_score = model.coef_(X_train, y_train)\n",
    "                temp_train_score_list.append(temp_train_score)\n",
    "            if name not in total_model_dict:\n",
    "                total_model_dict[name] = []\n",
    "            else:\n",
    "                total_model_dict[name].append(model)\n",
    "\n",
    "        print('Model: ' + name)\n",
    "        print('Test Score: ' + str(np.mean(temp_score_list)) + ' with STD: ' + str(np.std(temp_score_list)))\n",
    "        print('Train Score: ' + str(np.mean(temp_train_score_list)) + ' with STD: ' + str(np.std(temp_train_score_list)))\n",
    "    return total_model_dict\n",
    "\n",
    "\n",
    "def plot_overfit(X,y,model_obj,param_ranges,param_static=None): \n",
    "    '''\n",
    "    Function that outputs the plot of the hyper parameter versus the score.\n",
    "    '''\n",
    "    for parameter,parameter_range in param_ranges.items():\n",
    "        avg_train_score, avg_test_score = [],[]\n",
    "        std_train_score, std_test_score = [],[]\n",
    "        \n",
    "        for param_val in parameter_range:\n",
    "            param = {parameter:param_val}\n",
    "            if param_static:\n",
    "                param.update(param_static)\n",
    "                \n",
    "            model = model_obj(**param)\n",
    "            \n",
    "            train_scores,test_scores = [],[]\n",
    "            for i in range(5):\n",
    "                X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .3)\n",
    "                model.fit(X_train,y_train)\n",
    "                \n",
    "                train_scores.append(model.score(X_train,y_train))\n",
    "                test_scores.append(model.score(X_test,y_test))\n",
    "            \n",
    "            avg_train_score.append(np.mean(train_scores))\n",
    "            avg_test_score.append(np.mean(test_scores))\n",
    "            \n",
    "            std_train_score.append(np.std(train_scores))\n",
    "            std_test_score.append(np.std(test_scores))\n",
    "            \n",
    "        fig,ax = plt.subplots()\n",
    "        \n",
    "        if parameter == 'alpha':\n",
    "            ax.errorbar(np.log(parameter_range),avg_train_score,yerr=std_train_score,label='training score')\n",
    "            ax.errorbar(np.log(parameter_range),avg_test_score,yerr=std_test_score,label='testing score')\n",
    "            ax.set_xlabel('ln(' + parameter + ')', fontsize='14')\n",
    "        else:\n",
    "            ax.errorbar(parameter_range,avg_train_score,yerr=std_train_score,label='training score')\n",
    "            ax.errorbar(parameter_range,avg_test_score,yerr=std_test_score,label='testing score')  \n",
    "            ax.set_xlabel(parameter)\n",
    "            \n",
    "        ax.set_ylabel('score', fontsize='14')\n",
    "        ax.set_title(parameter + ' tuning', fontsize='16')\n",
    "        ax.legend(loc=0, frameon=True)\n",
    "        sns.set_style(\"white\")\n",
    "        sns.set_style('ticks')\n",
    "        sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "        ttl = ax.title\n",
    "        ttl.set_position([.5, 1.05])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basketball-reference Web Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the web scrape\n",
    "url_bball = 'http://www.basketball-reference.com/leagues/NBA_2016_totals.html'\n",
    "link_bball = requests.get(url_bball)\n",
    "page_bball = link_bball.text\n",
    "\n",
    "soup_bball = BeautifulSoup(page_bball, 'lxml')\n",
    "table_bball = soup_bball.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull column names from the table\n",
    "# th should be in the first tr tag\n",
    "col_loc_bball = table_bball.find('tr')\n",
    "cols_bball = col_loc_bball.find_all('th')\n",
    "\n",
    "cols_list_bball = []\n",
    "for i in range(len(cols_bball)):\n",
    "    temp_col_bball = cols_bball[i].get_text()\n",
    "    cols_list_bball.append(temp_col_bball)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull data from the table\n",
    "# td tags in the second through last tr tag should contain all of the data\n",
    "all_rows_bball = table_bball.find_all('tr')[1:]\n",
    "\n",
    "all_data_bball = []\n",
    "\n",
    "for i in range(len(all_rows_bball)):\n",
    "    row_bball = all_rows_bball[i].find_all('td')\n",
    "    data_bball = []\n",
    "    for j in range(len(row_bball)):\n",
    "        datapoint_bball = row_bball[j].get_text()\n",
    "        data_bball.append(datapoint_bball)\n",
    "    all_data_bball.append(data_bball)\n",
    "\n",
    "# Remove the first column name since it is just another index of the player list\n",
    "df_2016 = pd.DataFrame(all_data_bball, columns=cols_list_bball[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the above three steps into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the above steps into one function that can run through multiple urls\n",
    "\n",
    "# Get the advanced data from 2000 to 2016\n",
    "\n",
    "bball_ref_url = 'http://www.basketball-reference.com/leagues/NBA_{year}_advanced.html'\n",
    "start_year = 2000\n",
    "end_year = 2017\n",
    "\n",
    "df_bball = bball_scrape_data(bball_ref_url, start_year, end_year)\n",
    "df_bball.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "df_bball.to_csv('data/bball_ref_player_database.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the process for the non-advanced stats for players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bball_tot_url = 'http://www.basketball-reference.com/leagues/NBA_{year}_totals.html'\n",
    "bball_per_url = 'http://www.basketball-reference.com/leagues/NBA_{year}_per_game.html'\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2017\n",
    "\n",
    "df_bball_tot = bball_scrape_data(bball_tot_url, start_year, end_year)\n",
    "print(df_bball_tot.head())\n",
    "\n",
    "df_bball_per = bball_scrape_data(bball_per_url, start_year, end_year)\n",
    "print(df_bball_per.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframes to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "df_bball_tot.to_csv('data/bball_ref_player_database_tot.csv')\n",
    "df_bball_per.to_csv('data/bball_ref_player_database_per.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA Player Salaries Web Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the salary data from 2000 to 2017\n",
    "\n",
    "# Test the web scrape\n",
    "url_sal = 'http://www.espn.com/nba/salaries/_/year/2000/page/1/seasontype/3'\n",
    "link_sal = requests.get(url_sal)\n",
    "page_sal = link_sal.text\n",
    "\n",
    "soup_sal = BeautifulSoup(page_sal, 'lxml')\n",
    "table_sal = soup_sal.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull column names from the table\n",
    "# First tr tag contains td tags that are the column names\n",
    "col_loc_sal = table_sal.find('tr')\n",
    "cols_sal = col_loc_sal.find_all('td')\n",
    "\n",
    "cols_list_sal = []\n",
    "for i in range(len(cols_sal)):\n",
    "    temp_col_sal = cols_sal[i].get_text()\n",
    "    cols_list_sal.append(temp_col_sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull data from the table\n",
    "# td tags in every tr tag with the class 'evenrow' and 'oddrow' have the salary data\n",
    "all_rows_sal = table_sal.find_all('tr', class_ = ['evenrow', 'oddrow'])\n",
    "\n",
    "all_data_sal = []\n",
    "\n",
    "for i in range(len(all_rows_sal)):\n",
    "    row_sal = all_rows_sal[i].find_all('td')\n",
    "    data_sal = []\n",
    "    for j in range(len(row_sal)):\n",
    "        datapoint_sal = row_sal[j].get_text()\n",
    "        data_sal.append(datapoint_sal)\n",
    "    all_data_sal.append(data_sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "df_sal_2000 = pd.DataFrame(all_data_sal, columns=cols_list_sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salary_url = 'http://www.espn.com/nba/salaries/_/year/{year}/page/{page}/seasontype/3'\n",
    "start_year2 = 2000\n",
    "end_year2 = 2017\n",
    "start_page = 1\n",
    "end_page = 4\n",
    "\n",
    "df_sal = sal_scrape_data(salary_url, start_year2, end_year2, start_page, end_page)\n",
    "df_sal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "df_sal.to_csv('data/salary_database.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the player stats dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the player stats databases.\n",
    "df_csv2 = pd.read_csv('data/bball_ref_player_database.csv')\n",
    "df_csv3 = pd.read_csv('data/bball_ref_player_database_tot.csv')\n",
    "# df_csv4 = pd.read_csv('bball_ref_player_database_per.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the Unnamed and Rk columns\n",
    "df_adv = df_csv2.iloc[:,2:]\n",
    "df_tot = df_csv3.iloc[:,2:]\n",
    "# df_per = df_csv4.iloc[:,2:]\n",
    "\n",
    "# Remove the rows there the Tm column value is 'Tm'. This indicates that the column headers were repeated in the df.\n",
    "df_adv = df_adv[df_adv['Tm'] != 'Tm']\n",
    "df_tot = df_tot[df_tot['Tm'] != 'Tm']\n",
    "# df_per = df_per[df_per['Tm'] != 'Tm']\n",
    "\n",
    "# The Player column has asteriks and other markers to denote things about the player. \n",
    "# These markers will be removed since the Player column is needed in order to join the two databases in the future.\n",
    "df_adv['Player'] = df_adv['Player'].map(lambda x: re.sub(r\"\\*\", '', x))\n",
    "df_tot['Player'] = df_tot['Player'].map(lambda x: re.sub(r\"\\*\", '', x))\n",
    "# df_per['Player'] = df_per['Player'].map(lambda x: re.sub(r\"\\*\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some players played on multiple teams in the same year due to trades.\n",
    "# These players will have their stats averaged to one value.\n",
    "df_adv2 = df_adv.sort(columns=['Player','Yr','Pos','Tm'])\n",
    "df_adv2 = df_adv2.apply(pd.to_numeric, errors='ignore')\n",
    "df_adv2 = df_adv2.groupby(['Player','Yr','Pos','Tm']).mean().reset_index()\n",
    "\n",
    "df_tot2 = df_tot.sort(columns=['Player','Yr','Pos','Tm'])\n",
    "df_tot2 = df_tot2.apply(pd.to_numeric, errors='ignore')\n",
    "df_tot2 = df_tot2.groupby(['Player','Yr','Pos','Tm']).mean().reset_index()\n",
    "\n",
    "# df_per2 = df_per.sort(columns=['Player','Yr'])\n",
    "# df_per2 = df_per2.apply(pd.to_numeric, errors='ignore')\n",
    "# df_per2 = df_per2.groupby(['Player','Yr']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the salary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the salary csv file\n",
    "df_csv = pd.read_csv('data/salary_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the Unnamed: 0 column and the RK column.\n",
    "df_salary = df_csv.iloc[:,2:]\n",
    "\n",
    "# Edit the Name column so that only the names are left.\n",
    "df_salary['NAME'] = df_salary['NAME'].map(lambda x: re.sub(r\",\\s.+\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Edit the Salary column to an integer value.\n",
    "df_salary['SALARY'] = df_salary['SALARY'].map(lambda x: re.sub(r\"\\$\", '', x))\n",
    "df_salary['SALARY'] = df_salary['SALARY'].map(lambda x: re.sub(r\",\", '', x))\n",
    "df_salary['SALARY'] = df_salary['SALARY'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjust the Salary column to account for inflation.\n",
    "# Average rate of inflation between 2000 and 2016 was 2.15%\n",
    "df_salary['yr_diff'] = df_salary['Yr'].map(lambda x: 2017 - x)\n",
    "df_salary['salary_adj'] = df_salary.apply(lambda x: x['SALARY']*(1 + 0.0215)**x['yr_diff'], axis=1).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ultimately the salary dataframe will only need the Name, Year, Salary, and Adjusted Salary columns. \n",
    "# The Name and Yr columns will be used as the key when joining the two databases.\n",
    "df_salary_final = df_salary.loc[:,['NAME','Yr', 'SALARY', 'salary_adj']]\n",
    "df_salary_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join dataframes on a key formed by the year and name of the player.\n",
    "Inner join will be used since the players that don't have salary information will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since df_tot2 and df_adv2 have duplicate columns these columns will be excluded when merged.\n",
    "col_to_use1 = df_tot2.columns - df_adv2.columns\n",
    "df_combined1 = df_adv2.merge(df_tot2[col_to_use1], how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# The salary dataframe and the combined dataframe from above should be joined on the player name and year.\n",
    "df_combined2 = df_salary_final.merge(df_combined1, how='inner', left_on=['NAME', 'Yr'], right_on=['Player', 'Yr'])\n",
    "df_combined2 = df_combined2.drop('Player', axis=1)\n",
    "\n",
    "# 3P% is null when the 3P and 3PA values are 0.0 since the 3P% is equal to 3P/3PA.\n",
    "# The same applies to FT% and 2P%.\n",
    "# Replace the null with 0.\n",
    "df_combined2['3P%'] = df_combined2.loc[:,'3P%'].fillna(0)\n",
    "df_combined2['2P%'] = df_combined2.loc[:, '2P%'].fillna(0)\n",
    "df_combined2['FT%'] = df_combined2.loc[:, 'FT%'].fillna(0)\n",
    "df_combined2['3PAr'] = df_combined2.loc[:, '3PAr'].fillna(0)\n",
    "df_combined2['FTr'] = df_combined2.loc[:, 'FTr'].fillna(0)\n",
    "df_combined2['FG%'] = df_combined2.loc[:, 'FG%'].fillna(0)\n",
    "df_combined2['eFG%'] = df_combined2.loc[:, 'eFG%'].fillna(0)\n",
    "\n",
    "# Create dummy variables for position.\n",
    "pos_dict = {'C': 'C', 'PF': 'PF','SF': 'SF','PG':'PG','SG':'SG','C-PF': 'C','SF-SG':'SF','PG-SG':'PG',\n",
    "           'PG-SF':'PG','SG-SF':'SG','PF-C':'PF','PF-SF':'PF','SG-PG':'SG','SG-PF':'SG','SF-PF':'SF'}\n",
    "df_combined2['Pos'] = df_combined2['Pos'].map(pos_dict)\n",
    "df_combined2 = pd.get_dummies(df_combined2, columns=['Pos'])\n",
    "\n",
    "# There are columns with all null values. These columns will be dropped.\n",
    "df_final = df_combined2.dropna(axis=1, how='all')\n",
    "# df_final = df_final.dropna(axis=0, how='all')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('Length of combined df1: ', len(df_combined2))\n",
    "# print('Length of salary df: ', len(df_salary_final))\n",
    "# print(len(df_combined2.columns))\n",
    "# print(df_combined2.columns)\n",
    "# print(df_final.info())\n",
    "# df_combined2.head()\n",
    "# df_null = df_combined2[df_combined2['3P%'].isnull()]\n",
    "# df_null['3P%'] = df_null.loc[:,'3P%'].fillna(0)\n",
    "# df_null.iloc[0,25:40]\n",
    "# df_final.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file for easy reloading.\n",
    "df_final.to_csv('data/merged_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph a few features versus the adjusted salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('data/merged_df.csv')\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,12))\n",
    "\n",
    "# Win Shares vs Salary\n",
    "WS_list = df_final['WS'].tolist()\n",
    "salary_list = df_final['salary_adj'].tolist()\n",
    "\n",
    "ax[0,0].scatter(WS_list, salary_list)\n",
    "ax[0,0].set_title('Win Shares vs Salary')\n",
    "ax[0,0].set_xlabel('Win Shares')\n",
    "ax[0,0].set_ylabel('Salary ($)')\n",
    "\n",
    "# VORP vs Salary\n",
    "VORP_list = df_final['VORP'].tolist()\n",
    "salary_list = df_final['salary_adj'].tolist()\n",
    "\n",
    "ax[0,1].scatter(VORP_list, salary_list)\n",
    "ax[0,1].set_title('VORP vs Salary')\n",
    "ax[0,1].set_xlabel('VORP')\n",
    "ax[0,1].set_ylabel('Salary ($)')\n",
    "\n",
    "# PER vs Salary\n",
    "PER_list = df_final['PER'].tolist()\n",
    "salary_list = df_final['salary_adj'].tolist()\n",
    "\n",
    "ax[1,0].scatter(PER_list, salary_list)\n",
    "ax[1,0].set_title('PER vs Salary')\n",
    "ax[1,0].set_xlabel('PER')\n",
    "ax[1,0].set_ylabel('Salary ($)')\n",
    "\n",
    "# TS% vs Salary\n",
    "TS_list = df_final['TS%'].tolist()\n",
    "salary_list = df_final['salary_adj'].tolist()\n",
    "\n",
    "ax[1,1].scatter(VORP_list, salary_list)\n",
    "ax[1,1].set_title('TS% vs Salary')\n",
    "ax[1,1].set_xlabel('TS%')\n",
    "ax[1,1].set_ylabel('Salary ($)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_X_cols = ['Age', 'G', 'MP',\n",
    "       'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%',\n",
    "       'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM',\n",
    "       'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST', 'BLK',\n",
    "       'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF', 'PTS',\n",
    "       'STL', 'TOV', 'TRB', 'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG', 'Pos_SF',\n",
    "       'Pos_SG']\n",
    "\n",
    "\n",
    "# Separate the column names into % and total counts and further separate into offensive and defensive stats\n",
    "# Will also have a list of advanced metrics\n",
    "# Also, columns that are relatively irrelevant such as GS (Games started) and FTA/FT, 2PA/2P will be excluded\n",
    "off_perc_cols = ['salary_adj','TS%', 'AST%', 'TOV%','FG%','eFG%','ORB%']\n",
    "off_sh_perc_cols = ['salary_adj','2P%','3P%','FT%']\n",
    "off_count_cols = ['salary_adj','AST','TOV','ORB', 'PTS']\n",
    "\n",
    "def_perc_cols = ['salary_adj','DRB%','STL%']\n",
    "def_count_cols = ['salary_adj','DRB','STL']\n",
    "\n",
    "adv_cols = ['salary_adj','PER','OWS','DWS','WS','WS/48','BPM']\n",
    "\n",
    "common_cols = ['salary_adj','Age','PTS','AST','TRB','BLK','STL','TOV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common stats pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot1 = df_final.loc[:,common_cols]\n",
    "\n",
    "sns.pairplot(df_plot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced stats pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot2 = df_final.loc[:,adv_cols]\n",
    "\n",
    "sns.pairplot(df_plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive percentage stats pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot3 = df_final.loc[:,off_perc_cols]\n",
    "\n",
    "sns.pairplot(df_plot3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive count stats pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot4 = df_final.loc[:,off_count_cols]\n",
    "\n",
    "sns.pairplot(df_plot4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive shooting percentage pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot5 = df_final.loc[:,off_sh_perc_cols]\n",
    "\n",
    "sns.pairplot(df_plot5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defensive percentage stats pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot6 = df_final.loc[:,def_perc_cols]\n",
    "\n",
    "sns.pairplot(df_plot6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defensive count stats pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plot7 = df_final.loc[:,def_count_cols]\n",
    "\n",
    "sns.pairplot(df_plot7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First need to shuffle the dataframe to remove any ordering.\n",
    "df_test = df_final.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Square PER, AGE, GS, G, USG%\n",
    "df_test2 = square_features(df_test, ['PER','Age','GS','G','USG%'])\n",
    "\n",
    "# Cube Age, G, USG%\n",
    "df_test3 = cube_features(df_test2, ['Age','G','USG%'])\n",
    "\n",
    "# Create ratios: PTS/TOV, AST/TOV, STL/TOV, BLK/TRB, STL/AST\n",
    "df_test3['PTS/TOV'] = (df_test3['PTS']/df_test3['TOV']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['AST/TOV'] = (df_test3['AST']/df_test3['TOV']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['STL/TOV'] = (df_test3['STL']/df_test3['TOV']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['BLK/TRB'] = (df_test3['BLK']/df_test3['TRB']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['STL/AST'] = (df_test3['STL']/df_test3['AST']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "\n",
    "\n",
    "\n",
    "# Create ratios: WS/PER, OWS/PER, WS/OWS, BPM/OWS, BPM/WSper48\n",
    "df_test3['WS/PER'] = (df_test3['WS']/df_test3['PER']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['OWS/PER'] = (df_test3['OWS']/df_test3['PER']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['WS/OWS'] = (df_test3['WS']/df_test3['OWS']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['BPM/OWS'] = (df_test3['BPM']/df_test3['OWS']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "df_test3['BPM/WSper48'] = (df_test3['BPM']/df_test3['WS/48']).replace([np.inf, -np.inf], np.nan).fillna(value=0)\n",
    "\n",
    "# Create features that will rank players in each team and each year.\n",
    "rank_col_list = ['PTS','AST','TRB','BLK','STL','TOV','MP','WS','PER']\n",
    "group_col_list = ['Yr','Tm']\n",
    "\n",
    "df_ranked = rank_features(df_test3, group_col_list, rank_col_list)\n",
    "df_test4 = df_test3.merge(df_ranked, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# Change target feature to log(salary_adj)\n",
    "df_test4['salary_adj_ln'] = df_test4['salary_adj'].apply(np.log)\n",
    "\n",
    "X_cols = ['Age', 'G',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'BLK', 'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'PTS', 'STL', 'TOV', 'TRB', 'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG',\n",
    "       'Pos_SF', 'Pos_SG', 'PER_sq', 'Age_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk','WS_rk','PER_rk']\n",
    "\n",
    "# Create the X and y matrices. \n",
    "# Also, set apart 15% of the data for a holdout set.\n",
    "holdout = df_test4.iloc[0:403, :]\n",
    "\n",
    "df_test5 = df_test4.iloc[403:,:]\n",
    "\n",
    "X = df_test4.iloc[403:,:]\n",
    "X = X.loc[:, X_cols]\n",
    "y = df_test4.iloc[403:,:]\n",
    "y = y.loc[:, 'salary_adj']\n",
    "\n",
    "X_arr = X.as_matrix()\n",
    "y_arr = y.as_matrix()\n",
    "\n",
    "# df_test2.min()\n",
    "# df_test2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "scores = cross_val_score(reg, X, y, cv=10, scoring='r2')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move on to ridge, lasso, and elastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "models['LinReg'] = linear_model.LinearRegression()\n",
    "models['Ridge'] = linear_model.Ridge()\n",
    "models['Lasso_2'] = linear_model.Lasso(alpha=2)\n",
    "models['Lasso_02'] = linear_model.Lasso(alpha=.2)\n",
    "models['Lasso_002'] = linear_model.Lasso(alpha=.02)\n",
    "models['Lasso_20'] = linear_model.Lasso(alpha=20)\n",
    "models['Lasso_200'] = linear_model.Lasso(alpha=200)\n",
    "models['Lasso_2000'] = linear_model.Lasso(alpha=2000)\n",
    "models['Lasso_20000'] = linear_model.Lasso(alpha=20000)\n",
    "models['ElasticNet_2'] = linear_model.ElasticNet(alpha=2)\n",
    "models['ElasticNet_20'] = linear_model.ElasticNet(alpha=20)\n",
    "models['ElasticNet_200'] = linear_model.ElasticNet(alpha=200)\n",
    "models['ElasticNet_2000'] = linear_model.ElasticNet(alpha=2000)\n",
    "models['ElasticNet_20000'] = linear_model.ElasticNet(alpha=20000)\n",
    "models['DecisionTree'] = tree.DecisionTreeRegressor(max_depth=5, min_samples_split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the models and compare to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_list = fit_models(models, df_test5, X_cols, 'salary_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(abs(model_list['Lasso_20'][0].coef_), X_cols),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attempt 1: PER, VORP, BPM\n",
    "att_1 = ['PER','VORP','BPM']\n",
    "\n",
    "model_list_1 = fit_models(models, df_test5, att_1, 'salary_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attempt 2: PTS, AST, TRB, STL, BLK\n",
    "att_2 = ['PTS','AST','TRB', 'STL', 'BLK']\n",
    "\n",
    "model_list_2 = fit_models(models, df_test5, att_2, 'salary_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attempt 3: PTS, AST, TRB, PER, VORP, BPM\n",
    "att_3 = ['PTS','AST','TRB', 'PER','VORP','BPM']\n",
    "\n",
    "model_list_3 = fit_models(models, df_test5, att_3, 'salary_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attempt 4: All but eFG%, WS/48, WS, TS%, Pos_SG, Pos_SF, FG%, 2P%\n",
    "df_att4 = square_features(df_test5, ['PTS','AST','TRB', 'STL', 'BLK'])\n",
    "att_4 = ['Age', 'G',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'BLK', 'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'PTS', 'STL', 'TOV', 'TRB', 'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG',\n",
    "       'Pos_SF', 'Pos_SG', 'PER_sq', 'Age_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk']\n",
    "\n",
    "model_list_4 = fit_models(models, df_att4, att_4, 'salary_adj')\n",
    "sorted(zip(abs(model_list_4['Lasso_2'][0].coef_),att_4),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 5 random, noisy features to see if there are any features that perform worse than these random features\n",
    "rand_col_names = ['RAND_1','RAND_2','RAND_3','RAND_4','RAND_5']\n",
    "df_rf = rand_features(df_test5, rand_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_dict = {}\n",
    "rf_dict['RF_10'] = RandomForestRegressor(n_estimators=10, max_features='auto', min_samples_split=10)\n",
    "rf_dict['RF_30'] = RandomForestRegressor(n_estimators=30, max_features='auto', min_samples_split=10)\n",
    "rf_dict['RF_50'] = RandomForestRegressor(n_estimators=50, max_features='auto', min_samples_split=10)\n",
    "rf_dict['RF_100'] = RandomForestRegressor(n_estimators=100, max_features='auto', min_samples_split=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_cols = ['Age', 'G',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'BLK', 'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'PTS', 'STL', 'TOV', 'TRB', 'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG',\n",
    "       'Pos_SF', 'Pos_SG', 'PER_sq', 'Age_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk',\n",
    "       'RAND_1','RAND_2','RAND_3','RAND_4','RAND_5']\n",
    "rf_scores = fit_models(rf_dict, df_rf, rf_cols, 'salary_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_fi = rf_scores['RF_100'][0].feature_importances_\n",
    "list_means = sorted(zip(mean_fi,rf_cols),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_cols = ['PER_sq', 'PER', 'Age_sq', 'USG%', 'Age', 'GS', 'DRB%', 'GS_sq', 'G', 'RAND_4']\n",
    "rf_scores2 = fit_models(rf_dict, df_rf, rf_cols, 'salary_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_fi2 = rf_scores2['RF_100'][0].feature_importances_\n",
    "list_means2 = sorted(zip(mean_fi2,rf_cols),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_cols = ['Age', 'G',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG',\n",
    "       'Pos_SF', 'Pos_SG', 'PER_sq', 'Age_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_dict = {}\n",
    "gb_dict['GB_10'] = GradientBoostingRegressor(n_estimators=10)\n",
    "gb_dict['GB_50'] = GradientBoostingRegressor(n_estimators=50)\n",
    "gb_dict['GB_100'] = GradientBoostingRegressor(n_estimators=100)\n",
    "gb_dict['GB_150'] = GradientBoostingRegressor(n_estimators=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_models_list = fit_models(gb_dict, df_rf, gb_cols, 'salary_adj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to tune hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_cols_gb = ['Age','PTS','AST','TRB','BLK','STL','TOV','G','USG%','Age_sq']\n",
    "\n",
    "X_gb = df_rf.loc[:, gb_cols]\n",
    "y_gb = df_rf.loc[:, 'salary_adj']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gb, y_gb, test_size=0.2)\n",
    "\n",
    "gb_parameters = {'n_estimators': [10,50,100,200,400], \n",
    "                 'max_depth': [2,3,4,5,6], \n",
    "                 'min_samples_split': [5,10,20,40,60],\n",
    "                 'max_features': ['sqrt', 'auto']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(GradientBoostingRegressor(), param_grid=gb_parameters, n_jobs=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Best Parameters: ', clf.best_params_)\n",
    "print('Best Score: ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_gb = GradientBoostingRegressor(n_estimators=200, min_samples_split=40, max_features='auto', max_depth=5)\n",
    "best_gb.fit(X_train, y_train)\n",
    "train_score = best_gb.score(X_train, y_train)\n",
    "test_score = best_gb.score(X_test, y_test)\n",
    "print('Train Score: ', train_score)\n",
    "print('Test Score: ', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(best_gb.feature_importances_, X_train.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grad_cols = ['Age',\n",
    "#        'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "#        'BLK%', 'TOV%', 'USG%', 'OWS', 'WS/48', 'OBPM',\n",
    "#        'DBPM', 'BPM', '2P', '2P%', '2PA', '3P%', '3PA', 'AST',\n",
    "#        'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "#        'eFG%',\n",
    "#        'PER_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "#        'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "#        'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "#        'BPM/WSper48']\n",
    "\n",
    "# grad_cols = ['Age',\n",
    "#        'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "#        'BLK%', 'TOV%', 'USG%', 'OWS', 'WS/48', 'OBPM',\n",
    "#        'DBPM', 'BPM', '2P', '2P%', '2PA', '3P%', '3PA', 'AST',\n",
    "#        'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "#        'eFG%',\n",
    "#        'PER_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "#        'Age_cube', 'G_cube',\n",
    "#        'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "#        'BPM/WSper48']\n",
    "\n",
    "grad_cols = ['Age',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'BLK%', 'TOV%', 'USG%', 'OWS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', '2P', '2P%', '2PA', '3P%', '3PA', 'AST',\n",
    "       'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'eFG%',\n",
    "       'PER_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube']\n",
    "\n",
    "X_gb2 = df_rf.loc[:, grad_cols]\n",
    "y_gb2 = df_rf.loc[:, 'salary_adj']\n",
    "\n",
    "X_train_gb, X_test_gb, y_train_gb, y_test_gb = train_test_split(X_gb2, y_gb2, test_size=0.2)\n",
    "\n",
    "rfecv_gb = RFECV(best_gb, n_jobs=3)\n",
    "rfecv_gb.fit(X_train_gb, y_train_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfecv_gb.score(X_test_gb, y_test_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(rfecv_gb.ranking_, grad_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_cols = ['Age', 'G',\n",
    "       'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'FG', 'FG%', 'FT', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'Pos_PF',\n",
    "       'Pos_SG', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk']\n",
    "\n",
    "X_l = df_rf.loc[:, lasso_cols]\n",
    "y_l = df_rf.loc[:, 'salary_adj']\n",
    "\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_l, y_l, test_size=0.2)\n",
    "rfecv_lasso = RFECV(model_list_4['Lasso_200'][0], n_jobs=3)\n",
    "rfecv_lasso.fit(X_train_l, y_train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfecv_lasso.score(X_test_l, y_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(rfecv_lasso.ranking_, lasso_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_cols = ['Age', 'G',\n",
    "       'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%',\n",
    "       'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP', '2P', '2P%', '2PA', '3P', '3P%', '3PA', 'AST',\n",
    "       'BLK', 'DRB', 'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'GS', 'ORB', 'PF',\n",
    "       'PTS', 'STL', 'TOV', 'TRB', 'eFG%', 'Pos_C', 'Pos_PF', 'Pos_PG',\n",
    "       'Pos_SF', 'Pos_SG', 'PER_sq', 'Age_sq', 'GS_sq', 'G_sq', 'USG%_sq',\n",
    "       'Age_cube', 'G_cube', 'USG%_cube', 'PTS/TOV', 'AST/TOV', 'STL/TOV',\n",
    "       'BLK/TRB', 'STL/AST', 'WS/PER', 'OWS/PER', 'WS/OWS', 'BPM/OWS',\n",
    "       'BPM/WSper48', 'PTS_rk', 'AST_rk', 'TRB_rk', 'BLK_rk', 'STL_rk',\n",
    "       'TOV_rk', 'MP_rk','WS_rk','PER_rk']\n",
    "\n",
    "X_rf = df_rf.loc[:, rf_cols]\n",
    "y_rf = df_rf.loc[:, 'salary_adj']\n",
    "\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_features='auto', min_samples_split=10)\n",
    "rfecv_rf = RFECV(rf_model, n_jobs=3)\n",
    "rfecv_rf.fit(X_train_rf, y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfecv_rf.score(X_test_rf, y_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(rfecv_rf.ranking_, rf_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pipelines to choose the best model with tuned hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pipe = X.loc[:, X_cols]\n",
    "y_pipe = y\n",
    "\n",
    "pipe_dict = {}\n",
    "pipe_dict['gradient_boost'] = GradientBoostingRegressor()\n",
    "pipe_dict['random_forest'] = RandomForestRegressor()\n",
    "pipe_dict['lin_reg'] = linear_model.LinearRegression()\n",
    "pipe_dict['lasso'] = linear_model.Lasso()\n",
    "pipe_dict['ridge'] = linear_model.Ridge()\n",
    "\n",
    "\n",
    "param_dict = {}\n",
    "param_dict['gradient_boost'] = {'model__n_estimators': [10,100,300], \n",
    "                              'model__max_depth': [2,3,4,5], \n",
    "                              'model__min_samples_split': [5,10,20,40],\n",
    "                              'model__max_features': ['sqrt', 'auto']}\n",
    "param_dict['random_forest'] = {'model__n_estimators': [10,100,300],\n",
    "                             'model__max_depth': [2,3,4,5], \n",
    "                             'model__min_samples_split': [5,10,20,40],\n",
    "                             'model__max_features': ['sqrt', 'auto']}\n",
    "param_dict['lin_reg'] = {'model__fit_intercept': [True]}\n",
    "param_dict['lasso'] = {'model__alpha': [.01,.1,1,10,100]}\n",
    "param_dict['ridge'] = {'model__alpha' : [.01,.1,1,10,100]}\n",
    "param_dict['elastic'] = {'model__alpha': [.01,.1,1,10,100]}\n",
    "\n",
    "# feature_list = [('identity', IdentityTransform()),\n",
    "#                 ('poly', PolynomialFeatures(degree=3)),\n",
    "#                 ('pca', decomposition.PCA(n_components=2))]\n",
    "\n",
    "# feature_list = [('pca', decomposition.PCA(n_components=2))]\n",
    "feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = run_pipelines(pipe_dict, X_pipe, y_pipe, feature_list, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the predicted target values and the true target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_lasso_holdout = holdout[lasso_cols]\n",
    "y_lasso_holdout = holdout['salary_adj']\n",
    "\n",
    "lasso = linear_model.Lasso(alpha=100)\n",
    "lasso.fit(X_train_l.append(X_test_l), y_train_l.append(y_test_l))\n",
    "y_pred_l = lasso.predict(X_lasso_holdout)\n",
    "lasso.score(X_lasso_holdout, y_lasso_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(abs(lasso.coef_), lasso_cols), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig_l, ax_l = plt.subplots(1,1,figsize=(8,5))\n",
    "ax_l.scatter(y_lasso_holdout, y_pred_l, alpha=1)\n",
    "ax_l.plot(y_lasso_holdout, y_lasso_holdout)\n",
    "ax_l.set_title('Lasso: Predicted vs True Results', fontsize='16')\n",
    "ax_l.set_xlabel('True Results', fontsize='14')\n",
    "ax_l.set_ylabel('Predicted Results',fontsize='14')\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "\n",
    "ttl_l = ax_l.title\n",
    "ttl_l.set_position([.5, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_rf_holdout = holdout[rf_cols]\n",
    "y_rf_holdout = holdout['salary_adj']\n",
    "\n",
    "random_forest = RandomForestRegressor(min_samples_split=5, max_depth=5, max_features='auto', n_estimators=300)\n",
    "random_forest.fit(X_train_rf.append(X_test_rf), y_train_rf.append(y_test_rf))\n",
    "y_pred_rf = random_forest.predict(X_rf_holdout)\n",
    "random_forest.score(X_rf_holdout, y_rf_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(random_forest.feature_importances_, rf_cols), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_rf, ax_rf = plt.subplots(1,1,figsize=(8,5))\n",
    "ax_rf.scatter(y_rf_holdout, y_pred_rf, alpha=1)\n",
    "ax_rf.plot(y_rf_holdout, y_rf_holdout)\n",
    "ax_rf.set_title('RandomForest: Predicted vs True Results', fontsize='16')\n",
    "ax_rf.set_xlabel('True Results', fontsize='14')\n",
    "ax_rf.set_ylabel('Predicted Results',fontsize='14')\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "\n",
    "ttl_rf = ax_rf.title\n",
    "ttl_rf.set_position([.5, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_gb_holdout = holdout[grad_cols]\n",
    "y_gb_holdout = holdout['salary_adj']\n",
    "\n",
    "gradient_boost = GradientBoostingRegressor(max_depth=5, min_samples_split=20, max_features='sqrt', n_estimators=100)\n",
    "\n",
    "gradient_boost.fit(X_train_gb.append(X_test_gb), y_train_gb.append(y_test_gb))\n",
    "y_pred_gb = gradient_boost.predict(X_gb_holdout)\n",
    "gradient_boost.score(X_gb_holdout, y_gb_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(gradient_boost.feature_importances_, grad_cols), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,5))\n",
    "ax.scatter(y_gb_holdout, y_pred_gb)\n",
    "ax.plot(y_gb_holdout, y_gb_holdout)\n",
    "ax.set_xlabel('True Salaries', fontsize='14')\n",
    "ax.set_ylabel('Predicted Salaries',fontsize='14')\n",
    "ax.set_title('Predicted vs True Salaries',fontsize='16')\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "\n",
    "ttl_gb = ax.title\n",
    "ttl_gb.set_position([.5, 1.05])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the hyper parameters versus the test/training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overfit(X_gb_holdout, y_gb_holdout, GradientBoostingRegressor, {'max_features':range(1,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overfit(X_gb, y_gb, GradientBoostingRegressor, {'n_estimators':range(1,110,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overfit(X_gb, y_gb, GradientBoostingRegressor, {'min_samples_split':range(5,50,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overfit(X_gb, y_gb, linear_model.Lasso, {'alpha':[.01,.1,1,10,100,1000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PER vs Salary and highlight the outliers (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst = df_final[df_final['PER'] < -18]\n",
    "worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig_PER, ax_PER = plt.subplots(1,1,figsize=(8,5))\n",
    "\n",
    "PER_list = df_final['PER'].tolist()\n",
    "salary_list = df_final['salary_adj'].tolist()\n",
    "\n",
    "steph = df_final[df_final['NAME'] == 'Stephen Curry']\n",
    "steph_PER = steph['PER'].tolist()\n",
    "steph_sal = steph['salary_adj'].tolist()\n",
    "\n",
    "shaq = df_final[df_final['NAME'] == \"Shaquille O'Neal\"]\n",
    "shaq_PER = shaq['PER'].tolist()\n",
    "shaq_sal = shaq['salary_adj'].tolist()\n",
    "\n",
    "vin = df_final[df_final['PER'] < -18]\n",
    "vin_PER = vin['PER'].tolist()\n",
    "vin_sal = vin['salary_adj'].tolist()\n",
    "\n",
    "ax_PER.scatter(PER_list, salary_list, label='All Players', alpha=.1)\n",
    "ax_PER.scatter(steph_PER, steph_sal, color='red', label='Steph Curry', marker='v', s=40)\n",
    "ax_PER.scatter(shaq_PER, shaq_sal, color='green', label='Shaq', marker='s', s=40)\n",
    "ax_PER.scatter(vin_PER, vin_sal, color='purple', label='Vin Baker', marker='o', s=40)\n",
    "\n",
    "ax_PER.set_title('PER vs Salary', fontsize='16')\n",
    "ax_PER.set_xlabel('PER', fontsize='14')\n",
    "ax_PER.set_ylabel('Salary ($)', fontsize='14')\n",
    "ax_PER.legend(loc=2)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "sns.set_style({'legend.frameon': True})\n",
    "\n",
    "ttl_PER = ax_PER.title\n",
    "ttl_PER.set_position([.5, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average of 11 runs of the model with different holdout and train/test sets.\n",
    "l_avg = [.526,.45,.515,.44,.525,.433,.479,.539,.471,.456,.504]\n",
    "rf_avg = [.548,.468,.549,.503,.522,.454,.489,.573,.497,.52,.487]\n",
    "gb_avg = [.86,.789,.841,.827,.864,.820,.767,.833,.823,.827,.827]\n",
    "\n",
    "avgs = [l_avg, rf_avg, gb_avg]\n",
    "\n",
    "for avg in avgs:\n",
    "    print('AVG: ', np.mean(avg))\n",
    "    print('STD: ', np.std(avg))\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
